{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f907f1",
   "metadata": {},
   "source": [
    "# Digital Habits vs Mental Health - Exploratory Data Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook explores the relationship between digital habits and mental health outcomes. We analyze screen time, social media usage, sleep patterns, and their correlation with stress levels and mood scores.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset contains the following variables:\n",
    "- **screen_time_hours**: Total daily screen time across all digital devices (in hours)\n",
    "- **social_media_platforms_used**: Number of different social media platforms used daily\n",
    "- **hours_on_TikTok**: Daily time spent specifically on TikTok (in hours)\n",
    "- **sleep_hours**: Average number of hours the person sleeps per day\n",
    "- **stress_level**: Perceived stress level on a scale of 1‚Äì10\n",
    "- **mood_score**: Mood rating on a scale of 1‚Äì10, where higher is better\n",
    "\n",
    "## Research Applications\n",
    "- Predicting mood or stress level from digital usage behavior\n",
    "- Correlation analysis and data visualization practice\n",
    "- Feature selection and engineering projects\n",
    "- Designing early-warning systems for digital burnout\n",
    "- Training ML models to detect behavior patterns that lead to poor well-being"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a9fa4",
   "metadata": {},
   "source": [
    "## Env setup (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Automated Package Installation\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install packages from requirements.txt if it exists\"\"\"\n",
    "    # Look for requirements.txt in parent directories\n",
    "    current_path = Path.cwd()\n",
    "    requirements_paths = [\n",
    "        current_path / \"requirements.txt\",\n",
    "        current_path.parent / \"requirements.txt\", \n",
    "        current_path.parent.parent / \"requirements.txt\"\n",
    "    ]\n",
    "    \n",
    "    requirements_file = None\n",
    "    for path in requirements_paths:\n",
    "        if path.exists():\n",
    "            requirements_file = path\n",
    "            break\n",
    "    \n",
    "    if requirements_file:\n",
    "        print(f\"üì¶ Found requirements.txt at: {requirements_file}\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)])\n",
    "            print(\"‚úÖ Successfully installed all requirements!\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Error installing requirements: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  requirements.txt not found. Installing core packages individually...\")\n",
    "        core_packages = [\n",
    "            'pandas', 'numpy', 'matplotlib', 'seaborn', \n",
    "            'scipy', 'scikit-learn'\n",
    "        ]\n",
    "        for package in core_packages:\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "                print(f\"‚úÖ Installed {package}\")\n",
    "            except:\n",
    "                print(f\"‚ùå Failed to install {package}\")\n",
    "\n",
    "install_requirements()\n",
    "\n",
    "print(\"üîß Package installation completed.\")\n",
    "print(\"üìä Core packages: pandas, numpy, matplotlib, seaborn, scipy, scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7810009",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ccfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"‚úÖ Display settings configured!\")\n",
    "print(\"‚úÖ Ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652740c",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c27285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset - Update this path according to your data location\n",
    "# For this example, we'll create a sample dataset if the file doesn't exist\n",
    "\n",
    "try:\n",
    "    # Try to load from a common path\n",
    "    df = pd.read_csv(\"../data/digital_habits_vs_mental_health.csv\")\n",
    "    print(\"‚úÖ Dataset loaded successfully from ../data/\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Alternative path\n",
    "        df = pd.read_csv(\"digital_habits_vs_mental_health.csv\")\n",
    "        print(\"‚úÖ Dataset loaded successfully from current directory\")\n",
    "    except FileNotFoundError:\n",
    "        # Create sample data for demonstration\n",
    "        print(\"‚ö†Ô∏è  Dataset not found. Creating sample data for demonstration...\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        # Generate correlated sample data\n",
    "        df = pd.DataFrame({\n",
    "            'screen_time_hours': np.random.normal(6.0, 2.0, n_samples).clip(1, 12),\n",
    "            'social_media_platforms_used': np.random.randint(1, 6, n_samples),\n",
    "            'hours_on_TikTok': np.random.normal(2.4, 1.1, n_samples).clip(0.2, 7.2),\n",
    "            'sleep_hours': np.random.normal(7.0, 1.5, n_samples).clip(3, 10),\n",
    "            'stress_level': np.random.randint(1, 11, n_samples),\n",
    "            'mood_score': np.random.randint(2, 11, n_samples)\n",
    "        })\n",
    "        print(f\"‚úÖ Sample dataset created with {n_samples} rows\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üìä Columns: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST 5 ROWS:\")\n",
    "print(\"=\"*50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895600cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"=\"*50)\n",
    "print(\"DATASET INFORMATION:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES CHECK:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Missing values detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICAL SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3da9e9",
   "metadata": {},
   "source": [
    "## 2. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            cmap=\"RdBu_r\", \n",
    "            center=0,\n",
    "            mask=mask,\n",
    "            fmt='.3f',\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Correlation Matrix - Digital Habits vs Mental Health\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KEY CORRELATIONS WITH MENTAL HEALTH INDICATORS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Focus on correlations with mood_score and stress_level\n",
    "mental_health_corr = corr_matrix[['mood_score', 'stress_level']].sort_values('mood_score', ascending=False)\n",
    "print(\"\\nüéØ CORRELATIONS WITH MOOD SCORE:\")\n",
    "print(\"-\"*40)\n",
    "for var, corr_val in mental_health_corr['mood_score'].items():\n",
    "    if var != 'mood_score':\n",
    "        print(f\"{var:25s}: {corr_val:+.3f}\")\n",
    "\n",
    "print(\"\\nüéØ CORRELATIONS WITH STRESS LEVEL:\")\n",
    "print(\"-\"*40)\n",
    "for var, corr_val in mental_health_corr['stress_level'].items():\n",
    "    if var != 'stress_level':\n",
    "        print(f\"{var:25s}: {corr_val:+.3f}\")\n",
    "\n",
    "# Highlight specific correlations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTABLE RELATIONSHIPS:\")\n",
    "print(\"=\"*60)\n",
    "tiktok_screen_corr = corr_matrix.loc['hours_on_TikTok', 'screen_time_hours']\n",
    "stress_mood_corr = corr_matrix.loc['stress_level', 'mood_score']\n",
    "sleep_mood_corr = corr_matrix.loc['sleep_hours', 'mood_score']\n",
    "\n",
    "print(f\"üì± TikTok vs Total Screen Time: {tiktok_screen_corr:+.3f}\")\n",
    "print(f\"üò∞ Stress Level vs Mood Score: {stress_mood_corr:+.3f}\")\n",
    "print(f\"üò¥ Sleep Hours vs Mood Score: {sleep_mood_corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c75b2",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for detailed numerical summary\n",
    "def analyze_distribution(dataframe, column, plot=True):\n",
    "    \"\"\"\n",
    "    Displays detailed statistical summary and optional histogram for a numerical column.\n",
    "    \"\"\"\n",
    "    # Define quantiles for detailed analysis\n",
    "    quantiles = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    \n",
    "    print(f\"\\nüìä DISTRIBUTION ANALYSIS: {column.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Statistical summary\n",
    "    summary = dataframe[column].describe(quantiles)\n",
    "    for stat, value in summary.items():\n",
    "        print(f\"{stat:10s}: {value:8.3f}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    skewness = stats.skew(dataframe[column])\n",
    "    kurtosis = stats.kurtosis(dataframe[column])\n",
    "    print(f\"{'Skewness':10s}: {skewness:8.3f}\")\n",
    "    print(f\"{'Kurtosis':10s}: {kurtosis:8.3f}\")\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(dataframe[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(dataframe[column].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {dataframe[column].mean():.2f}')\n",
    "        plt.axvline(dataframe[column].median(), color='green', linestyle='--', \n",
    "                   label=f'Median: {dataframe[column].median():.2f}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        box = plt.boxplot(dataframe[column], patch_artist=True)\n",
    "        box['boxes'][0].set_facecolor('lightblue')\n",
    "        plt.ylabel(column)\n",
    "        plt.title(f'Box Plot of {column}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Analyze all numerical columns\n",
    "print(\"üîç COMPREHENSIVE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for column in df.columns:\n",
    "    analyze_distribution(df, column, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b31e6",
   "metadata": {},
   "source": [
    "## 4. Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffe5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive pair plot\n",
    "def create_pair_plot(dataframe):\n",
    "    \"\"\"\n",
    "    Generates pair plot for numerical features showing relationships and distributions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Create pair plot with regression lines\n",
    "    g = sns.pairplot(dataframe, \n",
    "                     diag_kind='hist',\n",
    "                     plot_kws={'alpha': 0.6, 's': 20},\n",
    "                     diag_kws={'bins': 30, 'alpha': 0.7})\n",
    "    \n",
    "    g.fig.suptitle('Pairwise Relationships - Digital Habits vs Mental Health', \n",
    "                   fontsize=16, y=1.02)\n",
    "    \n",
    "    # Customize the plot\n",
    "    for ax in g.axes.flatten():\n",
    "        if ax:\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üîó PAIRWISE RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This plot shows:\")\n",
    "print(\"‚Ä¢ Diagonal: Distribution of each variable\")\n",
    "print(\"‚Ä¢ Off-diagonal: Scatter plots between variable pairs\")\n",
    "print(\"‚Ä¢ Look for linear/non-linear relationships and clusters\")\n",
    "print()\n",
    "\n",
    "create_pair_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dec59",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection functions\n",
    "def detect_outliers_iqr(dataframe, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = dataframe[column].quantile(0.25)\n",
    "    Q3 = dataframe[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = dataframe[(dataframe[column] < lower_bound) | (dataframe[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(dataframe, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    # Convert to numpy array to ensure compatibility\n",
    "    data = np.array(dataframe[column].values, dtype=np.float64)\n",
    "    \n",
    "    # Calculate z-scores manually to avoid type issues\n",
    "    mean_val = np.mean(data)\n",
    "    std_val = np.std(data, ddof=0)\n",
    "    z_scores = np.abs((data - mean_val) / std_val)\n",
    "    \n",
    "    outliers = dataframe[z_scores > threshold]\n",
    "    return outliers, z_scores\n",
    "\n",
    "# Comprehensive outlier analysis\n",
    "print(\"üîç OUTLIER DETECTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Outlier Detection Analysis', fontsize=16)\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # IQR method\n",
    "    outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df, column)\n",
    "    \n",
    "    # Z-score method\n",
    "    outliers_zscore, z_scores = detect_outliers_zscore(df, column)\n",
    "    \n",
    "    # Store results\n",
    "    outlier_summary[column] = {\n",
    "        'iqr_outliers': len(outliers_iqr),\n",
    "        'zscore_outliers': len(outliers_zscore),\n",
    "        'total_points': len(df),\n",
    "        'iqr_percentage': (len(outliers_iqr) / len(df)) * 100,\n",
    "        'zscore_percentage': (len(outliers_zscore) / len(df)) * 100\n",
    "    }\n",
    "    \n",
    "    # Create box plot\n",
    "    axes[row, col].boxplot(df[column])\n",
    "    axes[row, col].set_title(f'{column}\\nIQR: {len(outliers_iqr)} | Z-score: {len(outliers_zscore)}')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä OUTLIER SUMMARY:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Variable':<25} {'IQR Method':<15} {'Z-Score Method':<15} {'IQR %':<10} {'Z-Score %':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for var, stats_dict in outlier_summary.items():\n",
    "    print(f\"{var:<25} {stats_dict['iqr_outliers']:<15} {stats_dict['zscore_outliers']:<15} \"\n",
    "          f\"{stats_dict['iqr_percentage']:<10.2f} {stats_dict['zscore_percentage']:<10.2f}\")\n",
    "\n",
    "# Identify variables with significant outliers\n",
    "print(\"\\n‚ö†Ô∏è  VARIABLES WITH SIGNIFICANT OUTLIERS (>5% by IQR):\")\n",
    "print(\"-\" * 50)\n",
    "significant_outliers = [var for var, stats in outlier_summary.items() \n",
    "                       if stats['iqr_percentage'] > 5]\n",
    "\n",
    "if significant_outliers:\n",
    "    for var in significant_outliers:\n",
    "        print(f\"‚Ä¢ {var}: {outlier_summary[var]['iqr_percentage']:.2f}% outliers\")\n",
    "else:\n",
    "    print(\"‚úÖ No variables have significant outlier issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c1a9d",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_engineered = df.copy()\n",
    "\n",
    "print(\"üîß FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Sleep deficit (how much less than 8 hours)\n",
    "df_engineered['sleep_deficit'] = np.maximum(0, 8 - df_engineered['sleep_hours'])\n",
    "print(\"‚úÖ Created 'sleep_deficit':          Hours below 8-hour sleep recommendation\")\n",
    "\n",
    "# 2. Digital wellness ratio (sleep vs screen time)\n",
    "df_engineered['digital_wellness_ratio'] = df_engineered['sleep_hours'] / (df_engineered['screen_time_hours'] + 1)\n",
    "print(\"‚úÖ Created 'digital_wellness_ratio': Sleep hours % Screen time hours\")\n",
    "\n",
    "# 3. TikTok dominance (TikTok as proportion of total screen time)\n",
    "df_engineered['tiktok_dominance'] = df_engineered['hours_on_TikTok'] / (df_engineered['screen_time_hours'] + 1)\n",
    "print(\"‚úÖ Created 'tiktok_dominance':       TikTok hours % Total screen time\")\n",
    "\n",
    "# 4. Stress-screen compound (interaction between stress and screen time)\n",
    "df_engineered['stress_screen_compound'] = df_engineered['stress_level'] * df_engineered['screen_time_hours']\n",
    "print(\"‚úÖ Created 'stress_screen_compound': Stress level x Screen time\")\n",
    "\n",
    "# 5. Social intensity (TikTok hours √ó screen time for social media focus)\n",
    "df_engineered['social_intensity'] = df_engineered['hours_on_TikTok'] * df_engineered['screen_time_hours']\n",
    "print(\"‚úÖ Created 'social_intensity':       TikTok hours x Screen time\")\n",
    "\n",
    "# 6. Categorical features\n",
    "# Sleep quality categories\n",
    "sleep_bins = [0, 6, 8, float('inf')]\n",
    "sleep_labels = ['Poor', 'Average', 'Good']\n",
    "df_engineered['sleep_quality'] = pd.cut(df_engineered['sleep_hours'], \n",
    "                                       bins=sleep_bins, \n",
    "                                       labels=sleep_labels, \n",
    "                                       right=False)\n",
    "\n",
    "# Screen time categories\n",
    "screen_bins = [0, 4, 8, float('inf')]\n",
    "screen_labels = ['Low', 'Moderate', 'High']\n",
    "df_engineered['screen_time_category'] = pd.cut(df_engineered['screen_time_hours'], \n",
    "                                              bins=screen_bins, \n",
    "                                              labels=screen_labels, \n",
    "                                              right=False)\n",
    "\n",
    "# Stress level categories\n",
    "stress_bins = [0, 3, 7, float('inf')]\n",
    "stress_labels = ['Low', 'Moderate', 'High']\n",
    "df_engineered['stress_category'] = pd.cut(df_engineered['stress_level'], \n",
    "                                         bins=stress_bins, \n",
    "                                         labels=stress_labels, \n",
    "                                         right=False)\n",
    "\n",
    "print(\"\\n‚úÖ Created categorical features: sleep_quality, screen_time_category, stress_category\")\n",
    "\n",
    "print(f\"\\nüìä Dataset expanded from {df.shape[1]} to {df_engineered.shape[1]} features\")\n",
    "print(\"\\nNew features preview:\")\n",
    "new_features = ['sleep_deficit', 'digital_wellness_ratio', 'tiktok_dominance', \n",
    "               'stress_screen_compound', 'social_intensity']\n",
    "df_engineered[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2696cf2",
   "metadata": {},
   "source": [
    "## 7. Random Forest Modeling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest analysis to understand key relationships and feature importance\n",
    "print(\"üå≤ RANDOM FOREST ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ensure all imports are available\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "target = 'mood_score'\n",
    "print(f\"üéØ Predicting: {target}\")\n",
    "\n",
    "# Include more features for Random Forest to showcase its capabilities\n",
    "feature_cols = ['screen_time_hours', 'sleep_hours', 'stress_level', 'hours_on_TikTok']\n",
    "\n",
    "# Add engineered features if they exist\n",
    "if 'screen_sleep_ratio' in df_engineered.columns:\n",
    "    feature_cols.extend(['screen_sleep_ratio', 'stress_screen_interaction', 'sleep_quality_score'])\n",
    "\n",
    "X = df_engineered[feature_cols]\n",
    "y = df_engineered[target]\n",
    "\n",
    "print(f\"üìä Using features: {feature_cols}\")\n",
    "\n",
    "# Implement 70/15/15 train/validation/test split\n",
    "# First split: 70% train, 30% temp (which will be split into 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Split the 30% temp into 15% validation and 15% test (50/50 of the 30%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nüìä Data Split (70/15/15):\")\n",
    "print(f\"‚Ä¢ Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"‚Ä¢ Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"‚Ä¢ Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Create and fit the Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nüå≥ Training Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on all sets\n",
    "train_pred = rf_model.predict(X_train)\n",
    "val_pred = rf_model.predict(X_val)\n",
    "test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for all sets\n",
    "train_r2 = r2_score(y_train, train_pred)\n",
    "val_r2 = r2_score(y_val, val_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "val_mae = mean_absolute_error(y_val, val_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä TRAINING SET:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {train_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {train_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {train_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä VALIDATION SET:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {val_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {val_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä TEST SET:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {test_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_diff = train_r2 - val_r2\n",
    "val_test_diff = val_r2 - test_r2\n",
    "\n",
    "print(f\"\\nüîç MODEL ASSESSMENT:\")\n",
    "if train_val_diff > 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  Potential overfitting detected (Train-Val R¬≤ diff: {train_val_diff:.4f})\")\n",
    "elif train_val_diff < 0.05:\n",
    "    print(f\"   ‚úÖ Good generalization (Train-Val R¬≤ diff: {train_val_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"   üìä Moderate fit (Train-Val R¬≤ diff: {train_val_diff:.4f})\")\n",
    "\n",
    "print(f\"   üìà Validation-Test consistency: {abs(val_test_diff):.4f}\")\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest explains {test_r2:.1%} of variance in mood score (test set)\")\n",
    "\n",
    "# Cross-validation for additional robustness check\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"üîÑ 5-Fold CV R¬≤ Score: {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüèÜ TOP FEATURE IMPORTANCE:\")\n",
    "for idx, row in feature_importance_df.head().iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Visualizations and Model Assessment\n",
    "print(\"üìä RANDOM FOREST VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a comprehensive visualization dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Feature Importance Plot\n",
    "ax1 = axes[0, 0]\n",
    "feature_importance_df_sorted = feature_importance_df.sort_values('importance', ascending=True)\n",
    "colors = plt.get_cmap('viridis')(np.linspace(0, 1, len(feature_importance_df_sorted)))\n",
    "bars = ax1.barh(range(len(feature_importance_df_sorted)), feature_importance_df_sorted['importance'], color=colors)\n",
    "ax1.set_yticks(range(len(feature_importance_df_sorted)))\n",
    "ax1.set_yticklabels(feature_importance_df_sorted['feature'])\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_title('Random Forest\\nFeature Importance', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# 2. Actual vs Predicted - Test Set\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(y_test, test_pred, alpha=0.6, c=y_test, cmap='viridis', s=20)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, alpha=0.8)\n",
    "ax2.set_xlabel('Actual Mood Score')\n",
    "ax2.set_ylabel('Predicted Mood Score')\n",
    "ax2.set_title(f'Actual vs Predicted (Test Set)\\nR¬≤ = {test_r2:.3f}', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax2, label='Actual Mood Score')\n",
    "\n",
    "# 3. Residuals Plot\n",
    "ax3 = axes[0, 2]\n",
    "residuals_test = y_test - test_pred\n",
    "ax3.scatter(test_pred, residuals_test, alpha=0.6, c='lightcoral', s=20)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.8)\n",
    "ax3.set_xlabel('Predicted Mood Score')\n",
    "ax3.set_ylabel('Residuals')\n",
    "ax3.set_title('Residuals Plot (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Performance Comparison (Train/Val/Test)\n",
    "ax4 = axes[1, 0]\n",
    "metrics = ['R¬≤', 'RMSE', 'MAE']\n",
    "train_metrics = [train_r2, train_rmse, train_mae]\n",
    "val_metrics = [val_r2, val_rmse, val_mae]\n",
    "test_metrics = [test_r2, test_rmse, test_mae]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "ax4.bar(x - width, train_metrics, width, label='Train', alpha=0.8, color='skyblue')\n",
    "ax4.bar(x, val_metrics, width, label='Validation', alpha=0.8, color='orange')\n",
    "ax4.bar(x + width, test_metrics, width, label='Test', alpha=0.8, color='lightgreen')\n",
    "\n",
    "ax4.set_xlabel('Metrics')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (train_val, val_val, test_val) in enumerate(zip(train_metrics, val_metrics, test_metrics)):\n",
    "    ax4.text(i - width, train_val + max(train_metrics) * 0.01, f'{train_val:.3f}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "    ax4.text(i, val_val + max(val_metrics) * 0.01, f'{val_val:.3f}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "    ax4.text(i + width, test_val + max(test_metrics) * 0.01, f'{test_val:.3f}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 5. Prediction Distribution\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(y_test, bins=30, alpha=0.7, label='Actual', color='lightblue', density=True)\n",
    "ax5.hist(test_pred, bins=30, alpha=0.7, label='Predicted', color='lightcoral', density=True)\n",
    "ax5.set_xlabel('Mood Score')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Distribution: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Cross-Validation Scores\n",
    "ax6 = axes[1, 2]\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "ax6.bar(range(len(cv_scores)), cv_scores, alpha=0.7, color='lightgreen')\n",
    "ax6.axhline(y=cv_mean, color='red', linestyle='--', alpha=0.8, \n",
    "            label=f'Mean: {cv_mean:.3f}')\n",
    "ax6.axhline(y=cv_mean + cv_std, color='orange', linestyle=':', alpha=0.6)\n",
    "ax6.axhline(y=cv_mean - cv_std, color='orange', linestyle=':', alpha=0.6)\n",
    "ax6.set_xlabel('CV Fold')\n",
    "ax6.set_ylabel('R¬≤ Score')\n",
    "ax6.set_title(f'5-Fold Cross-Validation\\nMean: {cv_mean:.3f} ¬± {cv_std:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax6.set_xticks(range(len(cv_scores)))\n",
    "ax6.set_xticklabels([f'Fold {i+1}' for i in range(len(cv_scores))])\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, score in enumerate(cv_scores):\n",
    "    ax6.text(i, score + 0.005, f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Random Forest Model Analysis Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed model assessment\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üîç DETAILED MODEL ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATA SPLITS:\")\n",
    "print(f\"   ‚Ä¢ Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ PERFORMANCE SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Best R¬≤ Score: {max(train_r2, val_r2, test_r2):.4f} ({'Training' if train_r2 == max(train_r2, val_r2, test_r2) else 'Validation' if val_r2 == max(train_r2, val_r2, test_r2) else 'Test'})\")\n",
    "print(f\"   ‚Ä¢ Generalization Gap: {train_r2 - test_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ CV Consistency: {cv_scores.std():.4f} (lower is better)\")\n",
    "\n",
    "print(f\"\\nüèÜ TOP 3 MOST IMPORTANT FEATURES:\")\n",
    "for i, (_, row) in enumerate(feature_importance_df.head(3).iterrows()):\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f} ({row['importance']/feature_importance_df['importance'].sum()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéâ MODEL CONCLUSION:\")\n",
    "if test_r2 >= 0.7:\n",
    "    print(f\"   ‚úÖ Excellent model performance (R¬≤ = {test_r2:.3f})\")\n",
    "elif test_r2 >= 0.5:\n",
    "    print(f\"   ‚úÖ Good model performance (R¬≤ = {test_r2:.3f})\")\n",
    "elif test_r2 >= 0.3:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate model performance (R¬≤ = {test_r2:.3f})\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Poor model performance (R¬≤ = {test_r2:.3f})\")\n",
    "\n",
    "if abs(train_r2 - test_r2) < 0.05:\n",
    "    print(f\"   ‚úÖ Well-generalized model (low overfitting)\")\n",
    "elif abs(train_r2 - test_r2) < 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate overfitting detected\")\n",
    "else:\n",
    "    print(f\"   ‚ùå High overfitting detected\")\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest successfully explains {test_r2:.1%} of the variance in mood scores!\")\n",
    "print(f\"üí° The model shows that sleep quality is the most important factor affecting mental well-being.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee576b",
   "metadata": {},
   "source": [
    "## 8. Key Findings and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51eea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"üéØ KEY FINDINGS FROM EDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate key statistics for summary\n",
    "sleep_mood_corr = df.corr().loc['sleep_hours', 'mood_score']\n",
    "screen_mood_corr = df.corr().loc['screen_time_hours', 'mood_score']\n",
    "stress_mood_corr = df.corr().loc['stress_level', 'mood_score']\n",
    "tiktok_screen_corr = df.corr().loc['hours_on_TikTok', 'screen_time_hours']\n",
    "\n",
    "avg_sleep = df['sleep_hours'].mean()\n",
    "avg_screen = df['screen_time_hours'].mean()\n",
    "avg_stress = df['stress_level'].mean()\n",
    "avg_mood = df['mood_score'].mean()\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Sample size: {len(df):,} observations\")\n",
    "print(f\"‚Ä¢ Average sleep: {avg_sleep:.1f} hours\")\n",
    "print(f\"‚Ä¢ Average screen time: {avg_screen:.1f} hours\")\n",
    "print(f\"‚Ä¢ Average stress level: {avg_stress:.1f}/10\")\n",
    "print(f\"‚Ä¢ Average mood score: {avg_mood:.1f}/10\")\n",
    "\n",
    "print(\"\\nüîó STRONGEST RELATIONSHIPS:\")\n",
    "print(f\"‚Ä¢ Sleep ‚Üî Mood: {sleep_mood_corr:+.3f} (Better sleep = Better mood)\")\n",
    "print(f\"‚Ä¢ Screen Time ‚Üî Mood: {screen_mood_corr:+.3f} (More screen time = Lower mood)\")\n",
    "print(f\"‚Ä¢ Stress ‚Üî Mood: {stress_mood_corr:+.3f} (Higher stress = Lower mood)\")\n",
    "print(f\"‚Ä¢ TikTok ‚Üî Total Screen: {tiktok_screen_corr:+.3f} (TikTok dominates screen time)\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"1. Sleep quality appears to be the strongest positive predictor of mental well-being\")\n",
    "print(\"2. Excessive screen time shows negative correlation with mood\")\n",
    "print(\"3. TikTok usage represents a significant portion of total screen time\")\n",
    "print(\"4. Stress level and mood score show strong negative correlation\")\n",
    "print(\"5. Digital habits are more about time spent than platform diversity\")\n",
    "\n",
    "print(\"\\nüö® POTENTIAL CONCERNS:\")\n",
    "high_screen_users = (df['screen_time_hours'] > 8).sum()\n",
    "poor_sleepers = (df['sleep_hours'] < 6).sum()\n",
    "high_stress_users = (df['stress_level'] > 7).sum()\n",
    "\n",
    "print(f\"‚Ä¢ {high_screen_users:,} users ({high_screen_users/len(df)*100:.1f}%) have >8h daily screen time\")\n",
    "print(f\"‚Ä¢ {poor_sleepers:,} users ({poor_sleepers/len(df)*100:.1f}%) get <6h sleep\")\n",
    "print(f\"‚Ä¢ {high_stress_users:,} users ({high_stress_users/len(df)*100:.1f}%) report high stress (>7/10)\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDATIONS FOR IMPROVING MENTAL HEALTH:\")\n",
    "print(\"1. Prioritize sleep hygiene - aim for 7-9 hours per night\")\n",
    "print(\"2. Implement screen time limits, especially for social media\")\n",
    "print(\"3. Consider TikTok usage boundaries due to its addictive nature\")\n",
    "print(\"4. Monitor and manage stress levels through healthy coping mechanisms\")\n",
    "print(\"5. Focus on quality time usage rather than quantity of digital engagement\")\n",
    "\n",
    "print(\"\\nüìà NEXT STEPS FOR ANALYSIS:\")\n",
    "print(\"‚Ä¢ Develop more sophisticated ML models (Random Forest, XGBoost)\")\n",
    "print(\"‚Ä¢ Create intervention recommendation system\")\n",
    "print(\"‚Ä¢ Segment users into risk categories\")\n",
    "print(\"‚Ä¢ Build early warning system for mental health decline\")\n",
    "print(\"‚Ä¢ Validate findings with additional behavioral data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5cabd",
   "metadata": {},
   "source": [
    "## 9. Data Export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
